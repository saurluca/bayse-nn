# Bayesian Neural Networks: Causal Learning at the Single Neuron Level

A breakthrough implementation demonstrating how single neurons can distinguish causation from correlation through dendritic computation and interventional learning.

## üß† Project Overview

This project explores a fundamental question in neuroscience and AI: **How can single neurons implement causal reasoning?**

Based on Judea Pearl's do-calculus and recent research on active dendrites, we've implemented and tested multiple approaches for neurons to discover causal relationships among their inputs, ultimately achieving a breakthrough with **contrastive learning**.

## üèÜ Key Achievement

**‚úÖ Breakthrough: 81% accuracy with perfect intervention robustness**

Our contrastive learning model successfully distinguishes causal from correlational factors:

- **81% prediction accuracy** (highest achieved)
- **100% intervention robustness** (0/100 failures)
- **98% confound rejection** capability

## üî¨ Research Foundation

### Core Concepts

1. **Causality and Do-Calculus** (Judea Pearl)
   - Causal Bayesian Networks with direct causal effects
   - Do-calculus for reasoning about interventions
   - Distinguishing cause from effect and controlling confounders

2. **Neurons as Causal Experimenters**
   - Traditional view: Neurons perform passive summation
   - **New paradigm**: Neurons actively test causal hypotheses
   - Dendritic computations as hypothesis testers

3. **Implementation Strategy**
   - Test inputs through interventions (force input on/off)
   - Observe neuron output changes
   - Update weights only for inputs with significant causal effects
   - Learn causal structure without prior knowledge

## üìÅ Project Structure

### Core Implementation

- `predictive_causal_neuron.py` - **üèÜ Breakthrough contrastive learning model**
- `main.py` - Original causal neuron implementation with intervention testing

### Experimental Models

- `advanced_dendrite_models.py` - Active dendrite and context-gated approaches
- `dendritic_causal_models.py` - Compartmentalized and hierarchical models
- `ultimate_causal_test.py` - Challenging test scenarios
- `controlled_causal_test.py` - Controlled experimental validation
- `learning_comparison.py` - Comparative analysis across models

### Documentation & Results

- `background.md` - Theoretical foundation and key concepts
- `results/` - Directory containing all experimental outputs
  - `EXPERIMENTAL_RESULTS.md` - Comprehensive research summary
  - `*.png` - Performance visualizations and mechanism diagrams

### Dependencies

- `pyproject.toml` - Project configuration
- `uv.lock` - Dependency lock file

## üöÄ Getting Started

### Prerequisites

- Python 3.8+
- UV package manager (or pip)

### Installation

```bash
# Clone the repository
git clone <repository-url>
cd bayse-nn

# Install dependencies
uv sync
# or with pip: pip install numpy matplotlib
```

### Running the Breakthrough Model

```bash
# Run the contrastive learning model (breakthrough achievement)
uv run python predictive_causal_neuron.py

# Test original causal neuron
uv run python main.py

# Compare different approaches
uv run python learning_comparison.py
```

## üìä Results Summary

| Model | Accuracy | Intervention Robustness | Causal Discovery |
|-------|----------|------------------------|------------------|
| Basic Dendritic | 58% | 51% | Poor |
| Branch Specialized | 51% | 54% | Poor |
| Predictive Coding | 75% | 69% | Moderate |
| **Contrastive Learning** | **81%** | **100%** | **Excellent** |

## üî¨ How It Works

### Contrastive Learning Mechanism

1. **Store Examples**: Maintains positive and negative examples in dendritic compartments
2. **Find Minimal Pairs**: Identifies cases that differ minimally but have different outcomes
3. **Discrimination Analysis**: Measures how well each factor discriminates between outcomes
4. **Causal Ranking**: Successfully ranks causal factors above correlational factors

### Key Innovation

The breakthrough came from implementing **contrastive comparison** where the neuron:

- Stores recent examples in dendritic compartments (using calcium dynamics)
- Compares similar situations with different outcomes
- Updates synaptic weights based on discriminative power
- Achieves robust causal discovery without complex networks

## üß¨ Biological Plausibility

Our models are based on known neural mechanisms:

- **Dendritic Memory**: Calcium dynamics store recent patterns
- **Contrastive Comparison**: Dendritic spikes during pattern comparison
- **Activity-Dependent Plasticity**: Learning modulated by causal evidence
- **Inhibitory Control**: Suppression of spurious correlations

## üîÆ Future Directions

### Research Extensions

- Multi-timescale dendritic dynamics
- Hierarchical causal discovery across neuron populations
- Integration with attention and working memory

### Applications

- Medical diagnosis systems with causal reasoning
- Scientific discovery tools
- AI systems that understand interventions

## üìñ Citation

If you use this work in your research, please cite:

```
Causal Learning at the Single Neuron Level: A Dendritic Computation Approach
[Your Name/Institution]
2024
```

## üìÑ License

[Add your license information here]

## ü§ù Contributing

Contributions are welcome! Please feel free to submit issues, feature requests, or pull requests.

---

*This research demonstrates that individual neurons are far more "intelligent" than traditionally believed, capable of implementing sophisticated causal reasoning through dendritic mechanisms.*
